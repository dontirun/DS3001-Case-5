{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Case Study 5 : Data Science in Email Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Required Readings:** \n",
    "* [Enron Emails](https://www.kaggle.com/wcukierski/enron-email-dataset) \n",
    "* Please download the Enron Email dataset from [here](https://www.cs.cmu.edu/~./enron/).\n",
    "* [TED Talks](https://www.ted.com/talks) for examples of 10 minutes talks.\n",
    "\n",
    "\n",
    "** NOTE **\n",
    "* Please don't forget to save the notebook frequently when working in Jupyter Notebook, otherwise the changes you made can be lost.\n",
    "\n",
    "*----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem: pick a data science problem that you plan to solve using Email Data\n",
    "* The problem should be important and interesting, which has a potential impact in some area.\n",
    "* The problem should be solvable using the data and data science solutions.\n",
    "\n",
    "Please briefly describe in the following cell: what problem are you trying to solve? why this problem is important and interesting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Since there was a scandal with Enron, we're looking at the top 5 executives with the most to do with this scandal. Gathering the topics and body most relevant to the scandal and analyzing the sentiment and any suspicion. Discussing things like \"Bankruptcy\", \"Fraud\", \"Shutdown\", etc.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration: Exploring the Email Dataset\n",
    "\n",
    "** plot email communication graph/network** \n",
    "* each node is an email account\n",
    "* the weight of an edge between two accounts depends on how many emails have been sent between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, sys, csv, datetime, json\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "csv.field_size_limit(sys.maxint)\n",
    "\n",
    "# Local path to data in project folder\n",
    "data_dir = 'data/'\n",
    "\n",
    "# Get each data file path\n",
    "data_files = [os.path.abspath(os.path.join(data_dir, file)) for file in os.listdir(data_dir)]\n",
    "\n",
    "# Store data\n",
    "comms_frequencies = dict()\n",
    "max_count = -sys.maxint - 1\n",
    "counter = 0\n",
    "\n",
    "# Read files\n",
    "for data_file in data_files:\n",
    "    with open(data_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader) # Skip header\n",
    "        \n",
    "        # Read each row\n",
    "        for row in reader:\n",
    "            message_dict = dict()\n",
    "            message_parts = row[1].split('\\n') # Split message by newline\n",
    "\n",
    "            # Parse line by line and store by message part key\n",
    "            for line in message_parts:\n",
    "                if line.startswith('From:'):\n",
    "                    message_dict['From'] = line[4:]\n",
    "                elif line.startswith('To:'):\n",
    "                    message_dict['To'] = line[2:]\n",
    "                elif line.startswith('X-To:'):\n",
    "                    message_dict['X-To'] = line[3:]\n",
    "                elif line.startswith('X-From:'):\n",
    "                    message_dict['X-From'] = line[6:]\n",
    "            \n",
    "            send_email = message_dict['From'].split('@')[0].strip()\n",
    "            \n",
    "            # Track only senders at Enron.com to save time/space\n",
    "            try:\n",
    "                send_domain = message_dict['From'].split('@')[1].strip()\n",
    "                if not send_domain == 'enron.com':\n",
    "                    continue\n",
    "            except IndexError:\n",
    "                # Different email format, continue anyway\n",
    "                pass\n",
    "            \n",
    "            recv_emails = None\n",
    "            \n",
    "            # Get all receivers of email in either field of email\n",
    "            if message_dict.has_key('To'):\n",
    "                recv_emails = [email.strip() for email in message_dict['To'].split(',')]\n",
    "            elif message_dict.has_key('X-To'):\n",
    "                recv_emails = [email.strip() for email in message_dict['X-To'].split(',')]\n",
    "            else:\n",
    "                # No senders found, skip\n",
    "                continue\n",
    "            \n",
    "            # Add counts to global dictionary\n",
    "            for recv_email in recv_emails:\n",
    "                if comms_frequencies.has_key(send_email):\n",
    "                    if comms_frequencies[send_email].has_key(recv_email):\n",
    "                        comms_frequencies[send_email][recv_email] += 1\n",
    "                    else:\n",
    "                        comms_frequencies[send_email][recv_email] = 1\n",
    "                else:\n",
    "                    comms_frequencies[send_email] = dict()\n",
    "                    comms_frequencies[send_email][recv_email] = 1\n",
    "            \n",
    "            # Store maximum number of emails between people for weighting\n",
    "            max_count = comms_frequencies[send_email][recv_email] if max_count < comms_frequencies[send_email][recv_email] else max_count\n",
    "\n",
    "            # Progress output\n",
    "            counter += 1            \n",
    "            if counter % 10000 == 0:\n",
    "                clear_output()\n",
    "                print 'Processed {} records.'.format(counter)\n",
    "\n",
    "clear_output()\n",
    "print 'Finished {} records. Max weight was {}.'.format(counter, max_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "\n",
    "# Write out data so we don't have to run it again\n",
    "output_data = {\n",
    "    'comms_frequencies': comms_frequencies\n",
    "}\n",
    "\n",
    "output_file = 'comms_frequencies'\n",
    "\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(output_data, f)\n",
    "    print 'Wrote data.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read data in again from file\n",
    "output_file = 'comms_frequencies'\n",
    "\n",
    "# Data structure\n",
    "comms_frequencies = None\n",
    "\n",
    "with open(output_file, 'r') as f:\n",
    "    output_data = pickle.load(f)\n",
    "    comms_frequencies = output_data['comms_frequencies']\n",
    "    print 'Loaded data.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "plt.figure(figsize=(200,200))\n",
    "G=nx.Graph()\n",
    "\n",
    "# Add only first 100 senders and all their connected nodes because it's unreadable after that and takes\n",
    "# too long to make a graph\n",
    "count = 0\n",
    "for send in comms_frequencies.keys():\n",
    "    for recv in comms_frequencies[send].keys():\n",
    "        G.add_edge(send, recv, weight=float(comms_frequencies[send][recv] / max_count))\n",
    "    \n",
    "    count += 1\n",
    "    \n",
    "    if count > 100:\n",
    "        break\n",
    "\n",
    "pos=nx.spring_layout(G)\n",
    "nx.draw_networkx_nodes(G,pos,node_size=2000)\n",
    "nx.draw_networkx_edges(G,pos,width=2)\n",
    "\n",
    "# labels\n",
    "nx.draw_networkx_labels(G,pos,font_size=6,font_family='sans-serif')\n",
    "\n",
    "plt.axis('off')\n",
    "plt.savefig(\"weighted_graph.png\") # Save graph as image to view better\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Solution: implement a data science solution to the problem you are trying to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly describe the idea of your solution to the problem in the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write codes to implement the solution in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are the top executives:\n",
    "\n",
    "# Kenneth Lay:\n",
    "# kenneth.lay@enron.com\n",
    "# Founder Chairman and CEO \n",
    "\n",
    "# Jeffery Skilling:\n",
    "# jeff.skilling@enron.com\n",
    "# Former President, and COO\n",
    "\n",
    "# Andrew Fastow:\n",
    "# andrew.fastow@enron.com\n",
    "# Former Chief Financial Officer \n",
    "\n",
    "# Rebecca Mark:\n",
    "# rebecca.mark@enron.com\n",
    "# Former Vice Chairman, Chairman and CEO of Enron International:\n",
    "\n",
    "# Stephen F. Cooper:\n",
    "# Interim CEO and CRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is initial setup to running our solution\n",
    "\n",
    "import os, sys, csv, datetime, json, re\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "csv.field_size_limit(sys.maxint)\n",
    "\n",
    "# Local path to data in project folder\n",
    "data_dir = 'data/'\n",
    "\n",
    "# Get each data file path\n",
    "data_files = [os.path.abspath(os.path.join(data_dir, file)) for file in os.listdir(data_dir)]\n",
    "\n",
    "# Store data\n",
    "max_count = -sys.maxint - 1\n",
    "counter = 0\n",
    "\n",
    "exec_emails = ['kenneth.lay@enron.com', 'jeff.skilling@enron.com', 'andrew.fastow@enron.com', 'rebecca.mark@enron.com']\n",
    "email_dict = dict()\n",
    "\n",
    "for email in exec_emails:\n",
    "    email_dict[email] = []\n",
    "\n",
    "print 'Setup complete.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell only if there is new data, otherwise load from file below\n",
    "\n",
    "# Used regex to isolate field, see reference here:\n",
    "# https://rforwork.info/2013/11/03/a-rather-nosy-topic-model-analysis-of-the-enron-email-corpus/\n",
    "x_filename_pat = re.compile(\"X-FileName:.+\\n\")\n",
    "to_pat = re.compile(\"To:.+\\n+\")\n",
    "xto_pat = re.compile(\"X-To:.+\\n\")\n",
    "cc_pat = re.compile(\"Cc:.+\\n\")\n",
    "bcc_pat = re.compile(\"Bcc:.+\\n\")\n",
    "xcc_pat = re.compile(\"X-cc:.+\\n\")\n",
    "xbcc_pat = re.compile(\"X-bcc:.+\\n\")\n",
    "mimver_pat = re.compile(\"Mime-Version:.+\\n\")\n",
    "ctype_pat = re.compile(\"Content-Type:.+\\n\")\n",
    "ctype_enc_pat = re.compile(\"Content-Transfer-Encoding:.+\\n\")\n",
    "xfolder_pat = re.compile(\"X-Folder:.+\\n\")\n",
    "xorigin_pat = re.compile(\"X-Origin:.+\\n\")\n",
    "from_pat = re.compile(\"From:.+\\n\")\n",
    "mess_id_pat = re.compile(\"Message-ID:.+\\n\")\n",
    "xfrom_pat = re.compile(\"X-From:.+\\n\")\n",
    "date_pat = re.compile(\"Date:.+\\n\")\n",
    "subject_pat = re.compile(\"Subject:.+\\n\")\n",
    "    \n",
    "# Read files\n",
    "for data_file in data_files:\n",
    "    with open(data_file, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader) # Skip header\n",
    "        \n",
    "        # Read each row\n",
    "        for row in reader:\n",
    "            email_new = x_filename_pat.sub('', row[1])\n",
    "            email_new = xfrom_pat.sub('', email_new)\n",
    "            email_new = xto_pat.sub('', email_new)\n",
    "            email_new = mimver_pat.sub('', email_new)\n",
    "            email_new = ctype_pat.sub('', email_new)\n",
    "            email_new = ctype_enc_pat.sub('', email_new)\n",
    "            email_new = xfolder_pat.sub('', email_new)\n",
    "            email_new = xorigin_pat.sub('', email_new)\n",
    "            email_new = mess_id_pat.sub('', email_new)\n",
    "            email_new = xcc_pat.sub('', email_new)\n",
    "            email_new = xbcc_pat.sub('', email_new)\n",
    "            email_new = cc_pat.sub('', email_new)\n",
    "            email_new = bcc_pat.sub('', email_new)\n",
    "            email_new = to_pat.sub('', email_new)\n",
    "            \n",
    "            from_field = from_pat.findall(email_new)[0].replace(\"From:\", \"\").strip()\n",
    "            email_new = from_pat.sub('', email_new)\n",
    "            \n",
    "            # If sender is one of top executives, store it\n",
    "            if from_field in exec_emails:\n",
    "                email_dict[from_field].append(email_new)\n",
    "            \n",
    "            # Progress output\n",
    "            counter += 1\n",
    "#             if counter > 0:\n",
    "#                 break\n",
    "            if counter % 10000 == 0:\n",
    "                clear_output()\n",
    "                print 'Processed {} records.'.format(counter)\n",
    "\n",
    "clear_output()\n",
    "print 'Finished {} records.'.format(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "\n",
    "# Write out data so we don't have to run it again\n",
    "output_data = {\n",
    "    'email_data': email_dict\n",
    "}\n",
    "\n",
    "output_file = 'email_data'\n",
    "\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(output_data, f)\n",
    "    print 'Wrote data.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Read data in again from file\n",
    "output_file = 'email_data'\n",
    "\n",
    "# Data structure\n",
    "email_dict = None\n",
    "\n",
    "with open(output_file, 'r') as f:\n",
    "    output_data = pickle.load(f)\n",
    "    email_dict = output_data['email_data']\n",
    "    print 'Loaded data.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "exec_word_freq = dict()\n",
    "\n",
    "for email in email_dict.keys():\n",
    "    if len(email_dict[email]) > 0:\n",
    "        if not exec_word_freq.has_key(email):\n",
    "            exec_word_freq[email] = dict()\n",
    "        \n",
    "        for email_text in email_dict[email]:\n",
    "            words = re.split(\"\\s+\", email_text)\n",
    "            \n",
    "            for word in words:\n",
    "                word = re.compile(\"^(.*?)\\@enron.com\").sub('', word)\n",
    "                if not exec_word_freq[email].has_key(word):\n",
    "                    exec_word_freq[email][word] = 0\n",
    "                exec_word_freq[email][word] += 1\n",
    "\n",
    "print 'Finished word frequencies.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The format of the word dict is as follows:\n",
    "```\n",
    "{\n",
    "    email: {\n",
    "        'word': 1,\n",
    "        'otherword': 20\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Please delete this cell when complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results: summarize and visualize the results discovered from the analysis\n",
    "\n",
    "Please use figures, tables, or videos to communicate the results with the audience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code starts here\n",
    "#   Please add comments or text cells in between to explain the general idea of each block of the code.\n",
    "#   Please feel free to add more cells below this cell if necessary\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*-----------------\n",
    "# Done\n",
    "\n",
    "All set! \n",
    "\n",
    "** What do you need to submit?**\n",
    "\n",
    "* **Notebook File**: Save this Jupyter notebook, and find the notebook file in your folder (for example, \"filename.ipynb\"). This is the file you need to submit. Please make sure all the plotted tables and figures are in the notebook. If you used \"jupyter notebook --pylab=inline\" to open the notebook, all the figures and tables should have shown up in the notebook.\n",
    "\n",
    "* **PPT Slides**: please prepare PPT slides (for 10 minutes' talk) to present about the case study . Each team present their case studies in class for 10 minutes.\n",
    "\n",
    "Please compress all the files in a zipped file.\n",
    "\n",
    "\n",
    "** How to submit: **\n",
    "\n",
    "        Please submit through Canvas, in the Assignment \"Case Study 5\".\n",
    "        \n",
    "** Note: Each team only needs to submit one submission in Canvas **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Peer-Review Grading Template:\n",
    "\n",
    "** Total Points: (100 points) ** Please don't worry about the absolute scores, we will rescale the final grading according to the performance of all teams in the class.\n",
    "\n",
    "Please add an \"**X**\" mark in front of your rating: \n",
    "\n",
    "For example:\n",
    "\n",
    "*2: bad*\n",
    "          \n",
    "**X** *3: good*\n",
    "    \n",
    "*4: perfect*\n",
    "\n",
    "\n",
    "    ---------------------------------\n",
    "    The Problem: \n",
    "    ---------------------------------\n",
    "    \n",
    "    1. (10 points) how well did the team describe the problem they are trying to solve using the data? \n",
    "       0: not clear\n",
    "       2: I can barely understand the problem\n",
    "       4: okay, can be improved\n",
    "       6: good, but can be improved\n",
    "       8: very good\n",
    "       10: crystal clear\n",
    "    \n",
    "    2. (10 points) do you think the problem is important or has a potential impact?\n",
    "        0: not important at all\n",
    "        2: not sure if it is important\n",
    "        4: seems important, but not clear\n",
    "        6: interesting problem\n",
    "        8: an important problem, which I want to know the answer myself\n",
    "       10: very important, I would be happy invest money on a project like this.\n",
    "    \n",
    "    ----------------------------------\n",
    "    Data Collection and Processing:\n",
    "    ----------------------------------\n",
    "    \n",
    "    3. (10 points) Do you think the data collected/processed are relevant and sufficient for solving the above problem? \n",
    "       0: not clear\n",
    "       2: I can barely understand what data they are trying to collect/process\n",
    "       4: I can barely understand why the data is relevant to the problem\n",
    "       6: the data are relevant to the problem, but better data can be collected\n",
    "       8: the data collected are relevant and at a proper scale\n",
    "      10: the data are properly collected and they are sufficient\n",
    "\n",
    "    -----------------------------------\n",
    "    Data Exploration:\n",
    "    -----------------------------------\n",
    "    4. How well did the team solve the following task:\n",
    "    \n",
    "    (1) plot email communication graph/network (10 points):\n",
    "       0: missing answer\n",
    "       4: okay, but with major problems\n",
    "       7: good, but with minor problems\n",
    "      10: perfect\n",
    "    \n",
    "\n",
    "    -----------------------------------\n",
    "    The Solution\n",
    "    -----------------------------------\n",
    "    5.  how well did the team describe the solution they used to solve the problem? (10 points)\n",
    "       0: not clear\n",
    "       2: I can barely understand\n",
    "       4: okay, can be improved\n",
    "       6: good, but can be improved\n",
    "       8: very good\n",
    "       10: crystal clear\n",
    "       \n",
    "    6. how well is the solution in solving the problem? (10 points)\n",
    "       0: not relevant\n",
    "       2: barely relevant to the problem\n",
    "       4: okay solution, but there is an easier solution.\n",
    "       6: good, but can be improved\n",
    "       8: very good, but solution is simple/old\n",
    "       10: innovative and technically sound\n",
    "       \n",
    "    7. how well did the team implement the solution in python? (10 points)\n",
    "       0: the code is not relevant to the solution proposed\n",
    "       2: the code is barely understandable, but not relevant\n",
    "       4: okay, the code is clear but incorrect\n",
    "       6: good, the code is correct, but with major errors\n",
    "       8: very good, the code is correct, but with minor errors\n",
    "      10: perfect \n",
    "   \n",
    "    -----------------------------------\n",
    "    The Results\n",
    "    -----------------------------------\n",
    "     8.  How well did the team present the results they found in the data? (10 points)\n",
    "       0: not clear\n",
    "       2: I can barely understand\n",
    "       4: okay, can be improved\n",
    "       6: good, but can be improved\n",
    "       8: very good\n",
    "      10: crystal clear\n",
    "       \n",
    "     9.  How do you think of the results they found in the data?  (5 points)\n",
    "       0: not clear\n",
    "       1: likely to be wrong\n",
    "       2: okay, maybe wrong\n",
    "       3: good, but can be improved\n",
    "       4: make sense, but not interesting\n",
    "       5: make sense and very interesting\n",
    "     \n",
    "    -----------------------------------\n",
    "    The Presentation\n",
    "    -----------------------------------\n",
    "    10. How all the different parts (data, problem, solution, result) fit together as a coherent story?  \n",
    "       0: they are irrelevant\n",
    "       1: I can barely understand how they are related to each other\n",
    "       2: okay, the problem is good, but the solution doesn't match well, or the problem is not solvable.\n",
    "       3: good, but the results don't make much sense in the context\n",
    "       4: very good fit, but not exciting (the storyline can be improved/polished)\n",
    "       5: a perfect story\n",
    "      \n",
    "    11. Did the presenter make good use of the 10 minutes for presentation?  \n",
    "       0: the team didn't present\n",
    "       1: bad, barely finished a small part of the talk\n",
    "       2: okay, barely finished most parts of the talk.\n",
    "       3: good, finished all parts of the talk, but some part is rushed\n",
    "       4: very good, but the allocation of time on different parts can be improved.\n",
    "       5: perfect timing and good use of time      \n",
    "\n",
    "    12. How well do you think of the presentation (overall quality)?  \n",
    "       0: the team didn't present\n",
    "       1: bad\n",
    "       2: okay\n",
    "       3: good\n",
    "       4: very good\n",
    "       5: perfect\n",
    "\n",
    "\n",
    "    -----------------------------------\n",
    "    Overall: \n",
    "    -----------------------------------\n",
    "    13. How many points out of the 100 do you give to this project in total?  Please don't worry about the absolute scores, we will rescale the final grading according to the performance of all teams in the class.\n",
    "    Total score:\n",
    "    \n",
    "    14. What are the strengths of this project? Briefly, list up to 3 strengths.\n",
    "       1: \n",
    "       2:\n",
    "       3:\n",
    "    \n",
    "    15. What are the weaknesses of this project? Briefly, list up to 3 weaknesses.\n",
    "       1:\n",
    "       2:\n",
    "       3:\n",
    "    \n",
    "    16. Detailed comments and suggestions. What suggestions do you have for this project to improve its quality further.\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    ---------------------------------\n",
    "    Your Vote: \n",
    "    ---------------------------------\n",
    "    1. [Overall Quality] Between the two submissions that you are reviewing, which team would you vote for a better score?  (5 bonus points)\n",
    "        0: I vote the other team is better than this team\n",
    "        5: I vote this team is better than the other team \n",
    "        \n",
    "    2. [Presentation] Among all the teams in the presentation, which team do you think deserves the best presentation award for this case study?  \n",
    "        1: Team 1\n",
    "        2: Team 2\n",
    "        3: Team 3\n",
    "        4: Team 4\n",
    "        5: Team 5\n",
    "        6: Team 6\n",
    "        7: Team 7\n",
    "        8: Team 8\n",
    "        9: Team 9\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
